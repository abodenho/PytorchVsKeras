{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Pytorch.constanteChooserValue import *\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from commun.CONSTANTE import *\n",
    "import torch\n",
    "from commun.CommunFonction import writeIntoFilesKeras\n",
    "from commun.datasets.dataManager import DataManager\n",
    "import torch.optim as optim\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "beginTime = time.time()\n",
    "data = DataManager(\"../commun/datasets/ANN/Churn_Modelling_traited.csv\")\n",
    "\n",
    "\n",
    "trainTensor,trainLabelTensor = data.getTrainCSV()\n",
    "testTensor, testLabelTensor = data.getTestCSV()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Traitement data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "trainTensor,trainLabelTensor = torch.tensor(trainTensor).float(),torch.tensor(trainLabelTensor).unsqueeze(1).float()\n",
    "testTensor, testLabelTensor  = torch.tensor(testTensor).float(),torch.tensor(testLabelTensor).unsqueeze(1).float()\n",
    "#unsqueeze(1) => augmente d'une dimmension | ixj => nxixj (n , i, j représentant des dimmension)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creation de batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def splitBatch(tensorToSplit,numberBatch):\n",
    "    lstBatch = []\n",
    "    sizeBatch = len(tensorToSplit)/numberBatch\n",
    "    for i in range(numberBatch):\n",
    "        supBorne = ceil(sizeBatch*(i+1))\n",
    "        infBorne = ceil(sizeBatch*i)\n",
    "        lstBatch.append(tensorToSplit[infBorne:supBorne][:])\n",
    "\n",
    "    return lstBatch\n",
    "\n",
    "trainBatch = splitBatch(trainTensor, BATCH_SIZE)\n",
    "trainBatchLabel = splitBatch(trainLabelTensor,BATCH_SIZE)\n",
    "testBatch = splitBatch(testTensor,BATCH_SIZE)\n",
    "testBatchLabel = splitBatch(testLabelTensor,BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Création modèle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(ANN_NUMBER_INPUT,NUMBER_NEURONE_HIDDEN_LAYER_ANN),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(NUMBER_NEURONE_HIDDEN_LAYER_ANN,NUMBER_NEURONE_HIDDEN_LAYER_ANN),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(NUMBER_NEURONE_HIDDEN_LAYER_ANN,NUMBER_NEURONE_HIDDEN_LAYER_ANN),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(NUMBER_NEURONE_HIDDEN_LAYER_ANN,NUMBER_NEURONE_OUTPUT),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = getOptimize(OPTIMIZER,model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def calculateAccuracy(result,target):\n",
    "    goodPredictionTensor = (result > THRESHOLD_TRUE) == target\n",
    "    total = len(goodPredictionTensor)\n",
    "    goodPrediction = 0\n",
    "    for booleanValue in goodPredictionTensor:\n",
    "        if booleanValue:\n",
    "            goodPrediction +=1\n",
    "    return goodPrediction/total\n",
    "\n",
    "def calculateMean(lstValue):\n",
    "    total = 0\n",
    "    for valueAcc in lstValue:\n",
    "        total += valueAcc\n",
    "    return total/len(lstValue)\n",
    "\n",
    "lstAcc = []\n",
    "lstValAcc = []\n",
    "lstLoss = []\n",
    "lstValLoss = []\n",
    "\n",
    "def trainLoop(numberEpoch,optimizer,model,lossFunction,\n",
    "              trainBatch,trainBatchLabel,\n",
    "              testBatch,testBatchLabel):\n",
    "\n",
    "    for epoch in range(1,numberEpoch+1):\n",
    "        subLstAcc = []\n",
    "        subLstAccVal = []\n",
    "        subLstLoss = []\n",
    "        subLstLossVal = []\n",
    "        for train,trainLabel,test,testLabel in zip(trainBatch,trainBatchLabel,\n",
    "                                                   testBatch,testBatchLabel):\n",
    "            resultTrain = model(train)\n",
    "            subLstAcc.append(calculateAccuracy(resultTrain,trainLabel))\n",
    "            lossTrain = lossFunction(resultTrain,trainLabel)\n",
    "            subLstLoss.append(lossTrain.item())\n",
    "            with torch.no_grad(): #désactive l'accumulation pour mise a jour des poids et biais\n",
    "                resultTest = model(test)\n",
    "                subLstAccVal.append(calculateAccuracy(resultTest,testLabel))\n",
    "                lossTest = lossFunction(resultTest,testLabel)\n",
    "                subLstLossVal.append(lossTest.item())\n",
    "\n",
    "            optimizer.zero_grad() #met a jour le buffer servant a l'accumulation\n",
    "            # pour la mise a jour poids et biais(époque précédente)\n",
    "            lossTrain.backward() #calcule le gradient des donnée\n",
    "            optimizer.step() #met a jours les données \n",
    "\n",
    "        lstAcc.append(calculateMean(subLstAcc))\n",
    "        lstValAcc.append(calculateMean(subLstAccVal))\n",
    "        lstLoss.append(calculateMean(subLstLoss))\n",
    "        lstValLoss.append(calculateMean(subLstLossVal))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "lossFunction = getLossFunction(FUNCTION_LOSS)\n",
    "\n",
    "trainLoop(NUMBER_EPOCH,optimizer,model,lossFunction,\n",
    "          trainBatch,trainBatchLabel,\n",
    "          testBatch, testBatchLabel)\n",
    "\n",
    "endTime = time.time() - beginTime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "writeIntoFilesKeras(lstAcc,lstValAcc,lstLoss,lstValLoss,endTime,\n",
    "                    f\"pytorch_{OPTIMIZER}_{NUMBER_NEURONE_HIDDEN_LAYER_ANN}_{FUNCTION_LOSS}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}